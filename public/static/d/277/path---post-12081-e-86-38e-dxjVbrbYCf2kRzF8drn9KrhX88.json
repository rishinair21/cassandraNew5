{"data":{"allAnantCassandralinks":{"edges":[{"node":{"title":"Eye or the Tiger: Benchmarking Cassandra vs. TimescaleDB for time-series data","alternative_id":12081,"content":"<p id=\"a3be\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">How a 5 node TimescaleDB cluster outperforms 30 Cassandra nodes, with higher inserts, up to 5800x faster queries, 10% the cost, a more flexible data model,¬†<br />and of course, full SQL.</em></p><figure id=\"6ea0\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*nez96S_THBQasVhTjWVacw.jpeg\" data-width=\"1024\" data-height=\"898\" data-action=\"zoom\" data-action-value=\"1*nez96S_THBQasVhTjWVacw.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*nez96S_THBQasVhTjWVacw.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">üé∂ It‚Äôs the </em></strong><a href=\"https://commons.wikimedia.org/wiki/File:Cassandra_logo.svg\" data-href=\"https://commons.wikimedia.org/wiki/File:Cassandra_logo.svg\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">eye</em></strong></a><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\"> or the </em></strong><a href=\"http://www.timescale.com/\" data-href=\"http://www.timescale.com/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">tiger</em></strong></a><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">, it‚Äôs the thrill of the fight (for time-series data)¬†üé∂</em></strong></figcaption></figure><p id=\"184d\" class=\"graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure\">With its simple <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">data partitioning and distribution architecture</a>, highly tunable <a href=\"https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" data-href=\"https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">consistency settings</a>, and strong cluster management <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsNodetool.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsNodetool.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tooling</a>, <strong class=\"markup--strong markup--p-strong\">Cassandra</strong> is legendary for its scalability. Developers often forego the expressiveness of SQL for the intoxicating power of being able to add write nodes to a Cassandra cluster with a single command. Moreover, Cassandra‚Äôs ability to provide sorted wide rows (more on this later) makes it a compelling use case for a scalable time-series data store.</p><p id=\"7899\" class=\"graf graf--p graf-after--p\"><a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">We‚Äôve already written</a> about how the notion of giving up the structure, maturity, and rich extensibility of PostgreSQL for scalability is a false dilemma. We‚Äôve also pitted TimescaleDB against another popular NoSQL database in <a href=\"https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016\" data-href=\"https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">our recent benchmarking post about MongoDB</a>. Nonetheless, Cassandra‚Äôs ease of use, staying power, and potential to handle time-series data well through its sequentially sorted wide rows make it a natural comparison to TimescaleDB.</p><p id=\"861f\" class=\"graf graf--p graf-after--p\">In this post, we dig deeper into using Cassandra vs. TimescaleDB for time-series workloads by comparing the scaling patterns, data model complexity, insert rates, read rates, and read throughput of each database. We start by comparing 5 node clusters for each database. Then, we benchmark a few different cluster configurations because the scaling properties of 5 node TimescaleDB and 5 node Cassandra are not perfectly analogous.</p><h4 id=\"9dfd\" class=\"graf graf--h4 graf-after--p\">Benchmarking Setup</h4><p id=\"0bf1\" class=\"graf graf--p graf-after--h4\">Let‚Äôs quickly take stock of the specs we used for these tests:</p><ul class=\"postList\"><li id=\"f9ba\" class=\"graf graf--li graf-after--p\">2 remote client machines, both on the same LAN as the databases</li><li id=\"b4f7\" class=\"graf graf--li graf-after--li\">Azure instances: Standard D8s v3 (8 vCPUs, 32 GB memory)</li><li id=\"3856\" class=\"graf graf--li graf-after--li\">5 TimescaleDB nodes, 5/10/30 Cassandra nodes (as noted)</li><li id=\"facc\" class=\"graf graf--li graf-after--li\">4 1-TB disks in a raid0 configuration (EXT4 filesystem)</li><li id=\"6880\" class=\"graf graf--li graf-after--li\">Dataset: 4,000 simulated devices generated 10 CPU metrics every 10 seconds for 3 full days (~100M reading intervals, ~1B metrics)</li><li id=\"5959\" class=\"graf graf--li graf-after--li\">For TimescaleDB, we set the chunk size to 12 hours, resulting in 6 total chunks (<a href=\"http://docs.timescale.com/v0.9/using-timescaledb/hypertables#best-practices\" data-href=\"http://docs.timescale.com/v0.9/using-timescaledb/hypertables#best-practices\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">more here</a>)</li></ul><h4 id=\"3ab3\" class=\"graf graf--h4 graf-after--li\">The Results</h4><p id=\"879a\" class=\"graf graf--p graf-after--h4\">Before moving forward, let‚Äôs start with a visual preview of how 5 TimescaleDB nodes fare against various sizes of Cassandra clusters:</p><figure id=\"9705\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" data-width=\"4056\" data-height=\"3449\" data-focus-x=\"51\" data-focus-y=\"23\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">5 node TimescaleDB cluster exhibits higher insert performance at a fraction of the cost (i.e., higher cluster efficiency) than a 30 node Cassandra cluster. Also, Cassandra scalability appears to be somewhat sub-linear.</strong></figcaption></figure><p id=\"e7db\" class=\"graf graf--p graf-after--figure\">Now, let‚Äôs look a bit further into how TimescaleDB and Cassandra achieve scalability.</p><h3 id=\"d2a6\" class=\"graf graf--h3 graf-after--p\">Scaling with TimescaleDB and Cassandra</h3><p id=\"aa2d\" class=\"graf graf--p graf-after--h3\">Cassandra provides simple scale-out functionality through a combination of its <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">data partitioning</a>, <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">virtual node abstraction</a>, and <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archGossipAbout.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archGossipAbout.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">internode gossip</a>. Adding a node to the cluster redistributes the data in a manner transparent to the client and increases write throughput more-or-less linearly (actually somewhat sub-linear in our tests)¬π. Cassandra also provides tunable availability with its <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">replication factor</a> configuration, which determines the number of nodes that have a copy of a given piece of data.</p><p id=\"5447\" class=\"graf graf--p graf-after--p\">PostgreSQL and TimescaleDB support scaling out reads by way of <a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">streaming replication</a>. Each replica node can be used as a read node to increase read throughput. Although PostgreSQL does not natively provide scale-out write functionality, users can often get the additional throughput they need by using RAID disk arrays or leveraging the <a href=\"https://www.postgresql.org/docs/current/static/manage-ag-tablespaces.html\" data-href=\"https://www.postgresql.org/docs/current/static/manage-ag-tablespaces.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tablespace</a> functionality provided by PostgreSQL. In addition, unlike PostgreSQL, TimescaleDB allows users to (elastically) assign multiple tablespaces to a single hypertable if desired (e.g., multiple network-attached disks), creating the potential for massively scaling disk throughput on a single TimescaleDB instance. Moreover, as we‚Äôll see, the write performance a single TimescaleDB instance provides for time-series data is quite often more than sufficient for a production workload‚Ää‚Äî‚Ääand that‚Äôs without some of the traditional NoSQL drawbacks that come with Cassandra.</p><h3 id=\"8e98\" class=\"graf graf--h3 graf-after--p\">Cassandra shortcomings: poor index support, no JOINs, restrictive query language, no referential integrity</h3><p id=\"0853\" class=\"graf graf--p graf-after--h3\">The dead simple scalability of Cassandra does not come without a cost. A number of key features that users of full-SQL databases like TimescaleDB take for granted are either not provided, very cumbersome, or not performant in Cassandra.</p><p id=\"563f\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Indexes: </strong>Unlike traditional SQL databases, Cassandra does not support global indexes, often making it prohibitive to look up or filter data by anything other than the primary or clustering keys. Clustering keys, which we‚Äôll discuss shortly, provide ordering only for a single ‚Äúrow‚Äù of data. Cassandra does support <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">secondary indexes</a>, but they are created <em class=\"markup--em markup--p-em\">locally</em> on each node to preserve the scaleable writes of Cassandra. This means that <em class=\"markup--em markup--p-em\">every</em> node must be queried each time an index lookup is performed, often leading to unacceptable performance.</p><p id=\"6192\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">JOINs</strong>: Cassandra is not a relational database and does not support natively joining data from two different sources. Users often have to create complex, error prone, and/or tedious client side logic to combine and filter data; indeed, this is what we had to do to get many of our benchmarking queries to be performant. While there are <a href=\"https://www.datastax.com/2015/03/how-to-do-joins-in-apache-cassandra-and-datastax-enterprise\" data-href=\"https://www.datastax.com/2015/03/how-to-do-joins-in-apache-cassandra-and-datastax-enterprise\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tools available</a> for performing joins in Cassandra, they rely on implementing client-side joins either through heavyweight clients or standalone proxies in front of Cassandra itself.</p><p id=\"d99a\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Query Language: </strong>Cassandra‚Äôs query language (<a href=\"http://cassandra.apache.org/doc/latest/cql/\" data-href=\"http://cassandra.apache.org/doc/latest/cql/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a>) lacks the expressiveness of SQL. Much of this has to do with some of the limitations brought on by Cassandra‚Äôs architecture. For example, the use of the <strong class=\"markup--strong markup--p-strong\">WHERE</strong> clause is limited to primary or clustering keys or fields that have secondary indexes defined on them¬≤, otherwise the coordinator would need to retrieve data from every node in the cluster for each query. This is also true of the <strong class=\"markup--strong markup--p-strong\">GROUPBY </strong>clause, which was only introduced in Cassandra 3.0. Another significant limitation for many application workflows is that you can only update data using its primary key. These and many other limitations make CQL a poor choice for workloads that require heavy analytical queries or data manipulation on fields beyond the primary and clustering keys.</p><p id=\"68f1\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Referential Integrity</strong>: There is no concept of referential integrity in Cassandra. Any constraints that you would typically model as a foreign key relationship in SQL are impossible in Cassandra without writing custom code to enforce them on the client side.</p><p id=\"f83f\" class=\"graf graf--p graf-after--p\">These limitations typically translate to more complicated data models on the server side and <em class=\"markup--em markup--p-em\">much</em> more complicated client-side logic than other databases we‚Äôve benchmarked. We‚Äôll see this on full display in the time-series data model we chose in order to make Cassandra as performant as possible.</p><p><a href=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\" data-href=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Towards 3B time-series data points per day: Why DNSFilter replaced InfluxDB with TimescaleDB</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Our results: 10x better resource utilization, even with 30% more requests</em>blog.dnsfilter.com</a></p><h3 id=\"387f\" class=\"graf graf--h3 graf-after--mixtapeEmbed\">Cassandra Data¬†Model</h3><p id=\"9321\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">First, a quick note on the origins of our Cassandra data model. In the interest of using a similar foundation for comparing database performance against time series workloads, we forked InfluxDB‚Äôs </em><a href=\"https://github.com/influxdata/influxdb-comparisons\" data-href=\"https://github.com/influxdata/influxdb-comparisons\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">benchmarker</em></a><em class=\"markup--em markup--p-em\"> for our own internal benchmarks. Their benchmarker comes equipped with a Cassandra time-series data model. We adopted their model largely as is, as we found their </em><a href=\"http://get.influxdata.com/rs/972-GDU-533/images/InfluxDB%201.4%20vs.%20Cassandra%20.pdf\" data-href=\"http://get.influxdata.com/rs/972-GDU-533/images/InfluxDB%201.4%20vs.%20Cassandra%20.pdf\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">reasoning</em></a><em class=\"markup--em markup--p-em\"> sufficiently compelling and aligned with </em><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">canonical approaches</em></a><em class=\"markup--em markup--p-em\"> to time-series data in Cassandra.</em></p><p id=\"c44e\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">That being said, we definitely do not consider ourselves Cassandra gurus, so we‚Äôre all ears if the community has suggestions on a better model that would improve our benchmarks. We plan to release our benchmarker in the coming weeks, and we‚Äôre eager to hear from any experts on various databases who would like to weigh in and help make our benchmarks more robust in general.</em></p><p id=\"8c06\" class=\"graf graf--p graf-after--p\">Cassandra is a <a href=\"https://en.wikipedia.org/wiki/Column_family\" data-href=\"https://en.wikipedia.org/wiki/Column_family\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">column family store</a>. Data for a column family, which is roughly analogous to a table in relational databases, is stored as a set of unique keys. Each of these keys maps to a set of columns which each contain the values for a particular data entry. These key-&gt;column-set tuples are called ‚Äúrows‚Äù (but should not be confused with rows in a relational database).</p><p id=\"a1b9\" class=\"graf graf--p graf-after--p\">In Cassandra, data is partitioned across nodes based on the column family key (called the <strong class=\"markup--strong markup--p-strong\">primary</strong> or <strong class=\"markup--strong markup--p-strong\">partition</strong> key). Additionally, Cassandra allows for <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompoundPrimaryKeyConcept.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompoundPrimaryKeyConcept.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">compound primary keys</a>, where the first key in the key definition is the primary/partition key, and any additional keys are known as <strong class=\"markup--strong markup--p-strong\">clustering keys. </strong>These clustering keys specify columns on which to sort the data for each row.</p><p id=\"40ed\" class=\"graf graf--p graf-after--p\">Let‚Äôs take a look at how this plays out with the dataset we use for our benchmarks. We simulate a devops monitoring use case where 4,000 unique hosts report 10 CPU metrics every 10 seconds over the course of 3 days, resulting in a 100 million row dataset<em class=\"markup--em markup--p-em\">.</em></p><p id=\"bc74\" class=\"graf graf--p graf-after--p\">In our Cassandra model, this translates to us creating a column family like this:</p><pre id=\"c399\" class=\"graf graf--pre graf-after--p\">CREATE TABLE measurements (<br />series_id text,<br />timestamp_ns bigint,<br />value double,<br />PRIMARY KEY(series_id, timestamp_ns));</pre><p id=\"efdf\" class=\"graf graf--p graf-after--pre\">The primary key, <strong class=\"markup--strong markup--p-strong\">series_id</strong>, is a combination of the host, day, and metric type in the format <strong class=\"markup--strong markup--p-strong\">hostname#metric_type#day</strong>. This allows us to get around some of the query limitations of Cassandra discussed above, particularly the weak support for joins, indexes, and server side rollups. By encoding the host, metric type, and day into the primary key, we can quickly and easily access the subset of data we need and execute any further filtering, aggregation, and grouping more performantly on the client side.</p><p id=\"c82f\" class=\"graf graf--p graf-after--p\">We use <strong class=\"markup--strong markup--p-strong\">timestamp_ns</strong> as our clustering key, which means that data for each row is ordered by timestamp as we insert it, providing optimal time range lookups. This is what a row of 3 values of the <strong class=\"markup--strong markup--p-strong\">cpu_guest </strong>metric for a given host on a given day would look like:</p><figure id=\"d8ae\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*F3sTKziDqcErVBefj_hsEA.png\" data-width=\"2366\" data-height=\"724\" data-action=\"zoom\" data-action-value=\"1*F3sTKziDqcErVBefj_hsEA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*F3sTKziDqcErVBefj_hsEA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">A Cassandra data model with measurements stored over¬†time</strong></figcaption></figure><p id=\"30f0\" class=\"graf graf--p graf-after--figure\">This is what we meant when we mentioned the <strong class=\"markup--strong markup--p-strong\">wide row approach</strong> earlier. Each row contains multiple columns, which are themselves sets of key-value pairs. The number of columns for a given row grows as we insert more readings corresponding to that row‚Äôs partition key. The columns are clustered by their timestamp, guaranteeing that each row will point to a sequentially sorted set of columns.</p><p id=\"e15a\" class=\"graf graf--p graf-after--p\">This ordered data is passed down to our custom client, which maintains a fairly involved <strong class=\"markup--strong markup--p-strong\">client-side index</strong> to perform the filtering and aggregation that is not supported in a performant manner by Cassandra‚Äôs secondary indexes. We maintain a data structure that essentially duplicates Cassandra‚Äôs primary key-&gt;metrics mapping and performs filtering and aggregations as we add data from our Cassandra queries. The aggregations and rollups we do on the client side are very simple (min, max, avg, groupby, etc.), so the vast majority of the query time remains at the database level. (In other words, the client-side index works, but also takes a lot more work.)</p><h3 id=\"f115\" class=\"graf graf--h3 graf-after--p\">Insert Performance</h3><p id=\"a7eb\" class=\"graf graf--p graf-after--h3\">Unlike TimescaleDB, Cassandra does not work well with large batch inserts. In fact, batching as a performance optimization is <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatch.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatch.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">explicitly discouraged</a> due to bottlenecks on the coordinator node if the transaction hits many partitions. Cassandra‚Äôs <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__advProps\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__advProps\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">default maximum batch size setting</a> is very small at 5KB. Nonetheless, we found that a small amount of batching (batches of 100) actually did help significantly with insert throughput for our dataset, so we used a batch size of 100 for our benchmarks.</p><p id=\"3e53\" class=\"graf graf--p graf-after--p\">To give Cassandra a fair shake against TimescaleDB, which allows for far larger batch sizes (we use 10,000 for our benchmarks), we ramped up the number of concurrent workers writing to Cassandra. While we used just 8 concurrent workers to maximize our write throughput on TimescaleDB, we used 1,800 concurrent workers (spread across multiple client machines) to max out our Cassandra throughput. We tested worker counts from 1 up to 1,800 before settling on 1,800 as the optimal number of workers for maximizing write throughput. Any number of workers above that caused unpredictable server side timeouts and negligible gains (in other words, the tradeoff of latency for throughput became unacceptable).</p><p id=\"1b4b\" class=\"graf graf--p graf-after--p\">To avoid client-side bottlenecks (e.g., with data serialization, the client-side index, or network overhead), we used 2 client VMs, each using our Golang benchmarker with 900 goroutines writing concurrently. We attempted to get more throughput by spreading the client load across even more VMs, but we found no improvements beyond 2 boxes.</p><p id=\"2835\" class=\"graf graf--p graf-after--p\">Since writes are sharded across nodes in Cassandra, its replication and consistency profile is a bit different than that of TimescaleDB. TimescaleDB writes all data to a single primary node which then replicates that data to any connected replicas through <a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">streaming replication</a>. Cassandra, on the other hand, shards the writes across the cluster, so no single replica stores all the cluster‚Äôs data. Instead, you define the <a href=\"http://cassandra.apache.org/doc/latest/architecture/dynamo.html#replication\" data-href=\"http://cassandra.apache.org/doc/latest/architecture/dynamo.html#replication\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">replication factor</a> for a given keyspace, which determines <em class=\"markup--em markup--p-em\">the number of nodes that will have a copy of each data item</em>. You can further<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlConfigConsistency.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlConfigConsistency.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"> control the consistency</a> of each write transaction on the client side by specifying how many nodes the client waits for the data to be written to. PostgreSQL and TimescaleDB similarly offer <a href=\"http://docs.timescale.com/v0.9/tutorials/replication#Configure-Replication-Parameters\" data-href=\"http://docs.timescale.com/v0.9/tutorials/replication#Configure-Replication-Parameters\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tunable consistency</a>.</p><p id=\"4fc0\" class=\"graf graf--p graf-after--p\">Given these significant differences, it‚Äôs difficult to achieve a truly apples-to-apples comparison of a 5 node TimescaleDB cluster vs. a 5 node Cassandra cluster. We decided on comparing a <strong class=\"markup--strong markup--p-strong\">TimescaleDB cluster with 1 primary and 4 read replicas, synchronous replication, and a </strong><a href=\"https://www.postgresql.org/docs/10/static/runtime-config-replication.html#synchronous_standby_names\" data-href=\"https://www.postgresql.org/docs/10/static/runtime-config-replication.html#synchronous_standby_names\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">consistency level of ANY 1</strong></a> against a <strong class=\"markup--strong markup--p-strong\">5 node Cassandra cluster with Replication Factor set to 2 and a consistency level of ONE</strong>. In both cases, clients will wait on data to be copied to 1 replica. Eventually data will be copied to 2 nodes in the case of Cassandra, while data will be copied to all nodes in the case of TimescaleDB.</p><p id=\"c3b0\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">In theory,</strong> then, Cassandra should have an advantage in insert performance since writes will be sharded across multiple nodes. On the read side, TimescaleDB should have a small advantage for very hot sets of keys (given they may be more widely replicated), but the total read throughput of the two should be theoretically comparable.</p><p id=\"ded7\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">In practice, however, we find that TimescaleDB has an advantage over Cassandra in both reads and writes, and <em class=\"markup--em markup--p-em\">it‚Äôs</em></strong><em class=\"markup--em markup--p-em\"> </em><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">large</em></strong><em class=\"markup--em markup--p-em\">.</em></p><p id=\"1c30\" class=\"graf graf--p graf-after--p\">Let‚Äôs take a look at the insert rates for each cluster:</p><figure id=\"0c24\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" data-width=\"4586\" data-height=\"2701\" data-action=\"zoom\" data-action-value=\"1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">5 TimescaleDB nodes outperforming 5 Cassandra nodes by 5.4x on¬†inserts</strong></figcaption></figure><p id=\"5beb\" class=\"graf graf--p graf-after--figure\">Despite Cassandra having the theoretical advantage of sharded writes, TimescaleDB exhibits <strong class=\"markup--strong markup--p-strong\">5.4x</strong> higher write performance than Cassandra. That actually understates the performance difference. Since TimescaleDB gets no write performance gains from adding extra nodes, we really only need a 3 node TimescaleDB cluster to achieve the same availability and write performance as our Cassandra cluster, making the real TimescaleDB performance multiplier closer to <strong class=\"markup--strong markup--p-strong\">7.6x</strong>.</p><p id=\"72d4\" class=\"graf graf--p graf-after--p\">This assumes that Cassandra scales perfectly linearly, which turns out to not quite be the case in our experience. We increased our Cassandra cluster to 10 then 30 nodes while keeping TimescaleDB at a cool 5 nodes:</p><figure id=\"7d18\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" data-width=\"1600\" data-height=\"942\" data-focus-x=\"50\" data-focus-y=\"39\" data-action=\"zoom\" data-action-value=\"1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">30 Cassandra nodes performing 27% slower than 5 TimescaleDB nodes</strong></figcaption></figure><p id=\"2ab7\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\">Even a 30 node Cassandra cluster performs nearly <em class=\"markup--em markup--p-em\">27% slower</em> for inserts against a single TimescaleDB primary.</strong> With 3 TimescaleDB nodes‚Ää‚Äî‚Ääthe maximum with TimescaleDB needed to provide the same availability as 30 node Cassandra with a Replication Factor of 2‚Ää‚Äî‚Ääwe now see that Cassandra needs well over <strong class=\"markup--strong markup--p-strong\">10x </strong>(probably closer to <strong class=\"markup--strong markup--p-strong\">15x</strong>) the resources as TimescaleDB to achieve similar write rates.</p><p id=\"229e\" class=\"graf graf--p graf-after--p\">For each node in these benchmarks, we paid for an Azure D8s v3 VM (<strong class=\"markup--strong markup--p-strong\">$616.85</strong>/month) as well as 4 attached 1TB SSDs (<strong class=\"markup--strong markup--p-strong\">$491.52</strong>/month). The minimum number of TimescaleDB nodes needed to achieve its write throughput and availability in the above chart is 3 (<strong class=\"markup--strong markup--p-strong\">$3,325.11</strong>/month), while the minimum number of Cassandra nodes required to achieve its highest write throughput and availability in the above chart is 30 (<strong class=\"markup--strong markup--p-strong\">$33,251.10</strong>/month). In other words, we paid <strong class=\"markup--strong markup--p-strong\">$29,925.99 </strong>more for Cassandra to get <strong class=\"markup--strong markup--p-strong\">73% </strong>as much write throughput as TimescaleDB.</p><p id=\"4b06\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Put another way, TimescaleDB exhibits higher inserts at 10% of the cost of Cassandra.</strong></p><figure id=\"599c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" data-width=\"1600\" data-height=\"605\" data-action=\"zoom\" data-action-value=\"1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">TimescaleDB: Higher inserts at 1/10 the cost of Cassandra</strong></figcaption></figure><h3 id=\"1735\" class=\"graf graf--h3 graf-after--figure\">Query Performance</h3><p id=\"0d71\" class=\"graf graf--p graf-after--h3\">Cassandra is admittedly less celebrated than SQL databases for its strength with analytical queries, but we felt it was worth diving into a few types of queries that come up frequently with time-series datasets. For all queries, we used 4 concurrent clients per node per query. We measured both the mean query times and the total read throughput in queries per second.</p><h4 id=\"037c\" class=\"graf graf--h4 graf-after--p\">Simple Rollups: TimescaleDB competitive (up to 4x¬†faster)</h4><p id=\"de89\" class=\"graf graf--p graf-after--h4\">We‚Äôll start with the mean query time on a few single rollup (i.e., groupby) queries on time. We ran these queries in 1000 different permutations (i.e., random time ranges and hosts).</p><figure id=\"14c2\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" data-width=\"4489\" data-height=\"2071\" data-action=\"zoom\" data-action-value=\"1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"600b\" class=\"graf graf--p graf-after--figure\">Cassandra holds its own here, but TimescaleDB is markedly better on 2 of the 3 simple rollup types and very competitive on the other. Even in simple time rollup queries where Cassandra‚Äôs clustering keys should really shine, TimescaleDB‚Äôs hypertables outperform.</p><p id=\"21b8\" class=\"graf graf--p graf-after--p\">Deducing the total read throughput of each database is fairly intuitive from the above chart, but let‚Äôs take a look at the recorded QPS of each queryset just to make sure there are no surprises:</p><figure id=\"d2f1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" data-width=\"4489\" data-height=\"2050\" data-action=\"zoom\" data-action-value=\"1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per second = BETTER performance</strong></figcaption></figure><p id=\"f714\" class=\"graf graf--p graf-after--figure\">When it comes to read throughput, TimescaleDB maintains its markedly better performance here.</p><h4 id=\"0a37\" class=\"graf graf--h4 graf-after--p\">Rollups on Time and Device: TimescaleDB 10x-47x¬†faster</h4><p id=\"e62e\" class=\"graf graf--p graf-after--h4\">Bringing multiple rollups (across both time and device) into the mix starts to make both databases sweat, but TimescaleDB has a huge advantage over Cassandra, especially when it comes to rolling up multiple metrics. Given the lengthy mean read times here, we only ran 100 for each query type.</p><figure id=\"283d\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"fdf0\" class=\"graf graf--p graf-after--figure\">We see a similar story for read throughput:</p><figure id=\"8b46\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" data-width=\"4489\" data-height=\"1782\" data-action=\"zoom\" data-action-value=\"1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per seconds = BETTER performance</strong></figcaption></figure><h4 id=\"a4fd\" class=\"graf graf--h4 graf-after--figure\">Complex Analytical Queries: TimescaleDB 3100x-5800x faster</h4><p id=\"ac39\" class=\"graf graf--p graf-after--h4\">We also took a look at 2 slightly more complex queries that you commonly encounter in time-series analysis.</p><p id=\"d00e\" class=\"graf graf--p graf-after--p\">The first (‚Äò<em class=\"markup--em markup--p-em\">lastpoint</em>‚Äô) is a query that retrieves the latest reading for every host in the dataset, even if you don‚Äôt a priori know <em class=\"markup--em markup--p-em\">when</em> it last communicated with the database¬≥.</p><p id=\"14ad\" class=\"graf graf--p graf-after--p\">The second (‚Äò<em class=\"markup--em markup--p-em\">groupby-orderby-limit</em>‚Äô) does a single rollup on time to get the MAX reading of a CPU metric on a per-minute basis for the last 5 intervals for which there are readings before a specified end time‚Å¥.</p><p id=\"7f7e\" class=\"graf graf--p graf-after--p\">Each queryset was run 100 times.</p><figure id=\"605c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"ada1\" class=\"graf graf--p graf-after--figure\">And the read throughput:</p><figure id=\"9ed0\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per second = BETTER performance</strong></figcaption></figure><p id=\"7054\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\">For these queries, Cassandra is clearly not the right tool for the job.</strong> TimescaleDB can easily leverage hypertables to narrow the search space to a single chunk, using a per-chunk index on host and time to gather our data from there. Our multi-part primary key on Cassandra, on the other hand, provides no guarantee that all of the data in a given time range will even be on a single node. In practice, for queries like this that touch every host tag in the data set, we end up scanning most, if not all, of the nodes in a cluster and grouping on the client side.</p><h3 id=\"45bd\" class=\"graf graf--h3 graf-after--p\">Conclusion</h3><p id=\"3211\" class=\"graf graf--p graf-after--h3\"><strong class=\"markup--strong markup--p-strong\">As we see, 5 TimescaleDB nodes outperform a 30 node Cassandra cluster, with higher inserts, up to 5800x faster queries, 10% the cost, a much more flexible data model, and full SQL.</strong></p><p id=\"868b\" class=\"graf graf--p graf-after--p\">Cassandra‚Äôs turnkey write scalability comes at a steep cost. For all but the simplest rollup queries, our benchmarks show TimescaleDB with a large advantage, with average query times anywhere from 10 to 5,873 times faster for common time-series queries. While Cassandra‚Äôs clustered wide rows provide good performance for querying data for a single key, it quickly degrades for complex queries involving multiple rollups across many rows.</p><p id=\"2014\" class=\"graf graf--p graf-after--p\">Additionally, while Cassandra makes it easy to add nodes to increase write throughput, it turns out you often just don‚Äôt need to do that for TimescaleDB. With 10‚Äì15x the write throughput of Cassandra, a single TimescaleDB node with a couple of replicas for high availability is more than adequate for dealing with workloads that would require a 30+ node fleet of Cassandra instances to handle.</p><p id=\"d47f\" class=\"graf graf--p graf-after--p\">However, Cassandra‚Äôs scaling model does offer nearly limitless storage since adding more storage capacity is as simple as adding another node to the cluster. A single instance of TimescaleDB currently tops out around 50‚Äì100TB. If you need to store petabyte scale data and can‚Äôt take advantage of retention policies or rollups, then massively clustered Cassandra might be the solution for you. However, we‚Äôre actively working on a clustered version of TimescaleDB that will similarly allow users to add nodes to increase write throughput and storage capacity‚Ää‚Äî‚Ääso please stay tuned for more details.</p><p id=\"52f0\" class=\"graf graf--p graf-after--p\">Also, we don‚Äôt claim to be Cassandra experts. We‚Äôve tried to be as open as we can about our data models, configurations, and methodologies so readers can raise any concerns they may have about our benchmarks and help us make them as accurate as possible.</p><p id=\"1422\" class=\"graf graf--p graf-after--p graf--trailing\">As a time-series database company we‚Äôll always be quite interested in evaluating the performance of other solutions. Cassandra was a pleasure to work with in terms of scaling out write throughput. But attaining that at the cost of per-node performance, the vibrant PostgreSQL ecosystem, and the expressiveness of full SQL simply does not seem worth it.</p>"}}]}},"pageContext":{"alternative_id":12081}}